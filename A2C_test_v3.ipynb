{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-island",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retained-flesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualSourcing(gym.Env):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \n",
    "        self.Lr = config['Lr']\n",
    "        self.Le = config['Le']\n",
    "        self.cr = config['cr']\n",
    "        self.ce = config['ce']\n",
    "        self.Lambda = config['lambda']\n",
    "        self.h = config['h']\n",
    "        self.b = config['b']\n",
    "        self.starting_state = config['starting_state']\n",
    "        self.max_order = config['max_order']\n",
    "        self.max_inventory = config['max_inventory']\n",
    "        \n",
    "        self.state = np.asarray(self.starting_state)\n",
    "        self.action_space = gym.spaces.MultiDiscrete([self.max_order+1]*2)\n",
    "        self.observation_space = gym.spaces.MultiDiscrete([self.max_order+1]*(self.Lr+self.Le)+[self.max_inventory])\n",
    "        \n",
    "        self.nA = (self.max_order+1) ** 2\n",
    "        \n",
    "        metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def seed(self, seed=None):\n",
    "        np.random.seed(seed)\n",
    "        self.action_space.np_random.seed(seed)\n",
    "        return seed\n",
    "    \n",
    "    def step(self, action):\n",
    "        reward = self.r(self.state)\n",
    "        \n",
    "        demand = np.random.poisson(self.Lambda)\n",
    "        newState = self.g(self.state, action)\n",
    "        newState[-1] = newState[-1] - demand\n",
    "        newState[-1] = max(-self.max_inventory, min(newState[-1], self.max_inventory))\n",
    "        self.state = newState.copy()\n",
    "        \n",
    "        return self.state, reward, demand, {}\n",
    "    \n",
    "    # Auxilary function computing the reward\n",
    "    def r(self, state):\n",
    "        return -(self.cr*state[self.Lr-1] + self.ce*state[self.Lr+self.Le-1] + \n",
    "                 self.h*max(state[-1],0) + self.b*max(-state[-1],0))\n",
    "    \n",
    "    # Auxilary function\n",
    "    def g(self, state, action):\n",
    "        return np.hstack([state[1:self.Lr], action[0], state[self.Lr+1:self.Lr+self.Le], action[1],\n",
    "                         state[self.Lr+self.Le]+state[0]+state[self.Lr]]).astype(int)\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        outfile = sys.stdout if mode == 'human' else super(DualSourcing, self).render(mode=mode)\n",
    "        outfile.write(np.array2string(self.state)+'\\n')\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.asarray(self.starting_state)\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-surrey",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, state_dim, action_space, hidden_size1):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_size1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.action_head = nn.Linear(hidden_size1, action_space)\n",
    "        self.action_output = nn.Softmax(dim = 0)\n",
    "        \n",
    "        self.value_output = nn.Linear(hidden_size1, 1)\n",
    "        \n",
    "        self.data = []\n",
    "\n",
    "    def forward(self, s):\n",
    "        out = self.fc1(s)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        action_probs = self.action_head(out)\n",
    "        action_probs = self.action_output(action_probs)\n",
    "        \n",
    "        value = self.value_output(out)\n",
    "        \n",
    "        return action_probs, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefbd39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convergence_test(env, numiters, policy, *args):\n",
    "\n",
    "    cum_reward = np.zeros(numiters)\n",
    "    av_reward = np.zeros(numiters)\n",
    "    \n",
    "    env.reset() # reset environment\n",
    "    for t in range(numiters-1):\n",
    "        action = policy(*args)\n",
    "        state, reward, demand, info = env.step(action)\n",
    "        cum_reward[t+1] = cum_reward[t] + reward\n",
    "        av_reward[t+1] = cum_reward[t+1] / (t+1)\n",
    "    print(av_reward[-1])\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(range(numiters), av_reward)\n",
    "    plt.xlabel('time step')\n",
    "    plt.ylabel('average reward')\n",
    "    plt.show()\n",
    "\n",
    "def evaluate(env, n_episodes, numiters, policy, *args):\n",
    "    \n",
    "    av_reward = np.zeros(n_episodes)\n",
    "    \n",
    "    for i in range(n_episodes):\n",
    "        av_r = 0\n",
    "        env.reset() # reset environment\n",
    "        for t in range(numiters):\n",
    "            action = policy(*args)\n",
    "            state, reward, demand, info = env.step(action)\n",
    "            if t > 100 and np.abs( av_r / (t+1) - (av_r + reward) / (t+2))  < 1e-4: # convergence is spotted\n",
    "                break\n",
    "            av_r = av_r + reward\n",
    "        av_reward[i] = av_r / (t+1)\n",
    "        \n",
    "    return np.mean(av_reward), np.std(av_reward) # return average reward and std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-integer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_policy(env, model, m):\n",
    "    state = torch.from_numpy(env.state).float()\n",
    "    action_probs, value = model(state)\n",
    "    \n",
    "    dist = Categorical(action_probs)\n",
    "    action = dist.sample()\n",
    "    model.data.append([dist.log_prob(action), value])\n",
    "    \n",
    "    action = np.asarray([action.item() // m, action.item() % m])\n",
    "    return action\n",
    "\n",
    "def TBS(env, r, S):\n",
    "    ip = np.sum(env.state[1:(env.Le+1)]) + np.sum(env.state[env.Lr:])\n",
    "    return r, max(0, S-ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e1afbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(env, model, r, S, stepsize=0.01, maxit=10000, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    m = env.max_order + 1\n",
    "    n = env.Lambda + 1\n",
    "    l = 2 * env.max_order + 1\n",
    "    \n",
    "    for i in range(maxit):\n",
    "        s = np.random.rand(state_dim)\n",
    "        s = np.floor(s * np.asarray([n]*(state_dim-1) + [l]))\n",
    "        s[-1] -= env.max_order\n",
    "        env.state = s\n",
    "\n",
    "        qr, qe = TBS(env, r, S)\n",
    "        action = m * qr + qe\n",
    "        action = int(action)\n",
    "        probs, _ = model(torch.from_numpy(s).float())\n",
    "        \n",
    "        model.zero_grad()\n",
    "        loss = (probs ** 2).sum() - 2 * probs[action]\n",
    "        loss.backward()\n",
    "\n",
    "        for name, layer in model.named_modules():\n",
    "            if type(layer) == nn.Linear and not (name == 'value_output'):\n",
    "                layer.weight.data -= stepsize * layer.weight.grad\n",
    "                layer.bias.data -= stepsize * layer.bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e6e072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def A2C(env, model, optimizer, maxit, rollout, param = {}):\n",
    "    if 'gamma' in param:\n",
    "        gamma = param['gamma']\n",
    "    else:\n",
    "        gamma = 0.99\n",
    "    \n",
    "    if 'terminal_state' in param:\n",
    "        terminal_state = param['terminal_state']\n",
    "    else:\n",
    "        terminal_state = None\n",
    "    \n",
    "    if 'loss_function' in param:\n",
    "        loss_function = param['loss_function']\n",
    "    else:\n",
    "        loss_function = functional.smooth_l1_loss\n",
    "    \n",
    "    if 'value_weight' in param:\n",
    "        value_weight = param['value_weight']\n",
    "    else:\n",
    "        value_weight = 1.0\n",
    "    \n",
    "    if 'number_of_actors' in param:\n",
    "        number_of_actors = param['number_of_actors']\n",
    "    else:\n",
    "        number_of_actors = 1\n",
    "    \n",
    "    if 'proportion' in param:\n",
    "        proportion = param['proportion']\n",
    "    else:\n",
    "        proportion = 1\n",
    "    \n",
    "    if 'env_seed' in param:\n",
    "        env.seed(param['env_seed'])\n",
    "    if 'torch_seed' in param:\n",
    "        torch.manual_seed(param['torch_seed'])\n",
    "\n",
    "    m = env.max_order + 1\n",
    "    \n",
    "    for i in range(maxit):\n",
    "        policy_loss = []\n",
    "        value_loss = []\n",
    "        for k in range(number_of_actors):\n",
    "            state = env.reset()\n",
    "            model.data = []\n",
    "            rewards = []\n",
    "            for t in range(rollout):\n",
    "                action = nn_policy(env, model, m)\n",
    "                state, reward, _, _ = env.step(action)\n",
    "                rewards.append(torch.tensor(reward).float())\n",
    "\n",
    "                if np.array_equal(state, terminal_state):\n",
    "                    break\n",
    "            \n",
    "            data = model.data\n",
    "            discounted_reward = 0\n",
    "            T = len(rewards)\n",
    "            for i in range(T-1, -1, -1):\n",
    "                log_prob, value = data[i]\n",
    "                discounted_reward = rewards[i] + gamma * discounted_reward\n",
    "                if i < T * proportion:\n",
    "                    advantage = discounted_reward - value.item()\n",
    "                    policy_loss.append(- advantage * log_prob / number_of_actors)\n",
    "                    value_loss.append(loss_function(value, torch.tensor([discounted_reward])) / number_of_actors)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = torch.stack(policy_loss).sum() + value_weight * torch.stack(value_loss).sum()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-kingdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {'Lr': 5, 'Le': 1, 'cr': 100, 'ce': 105, 'lambda': 10,\n",
    "          'h': 1, 'b': 19, 'starting_state': [0]*7, 'max_order': 20, 'max_inventory': 1000}\n",
    "\n",
    "# env = gym.make('DualSourcing-v0', config=CONFIG)\n",
    "env = DualSourcing(CONFIG)\n",
    "env.seed(0)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_space = env.nA\n",
    "\n",
    "torch.manual_seed(0)\n",
    "learning_rate = 0.01\n",
    "\n",
    "model = Model(state_dim, action_space, 32)\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "r = 5\n",
    "S = 3\n",
    "initialize(env, model, r, S)\n",
    "\n",
    "m = env.max_order + 1\n",
    "convergence_test(env, 10000, nn_policy, env, model, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de6a85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxit = 100\n",
    "rollout = 1000\n",
    "param = {'env_seed': 0, 'torch_seed': 0, 'number_of_actors': 1, 'proportion': 0.5}\n",
    "\n",
    "A2C(env, model, optimizer, maxit, rollout, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638c3ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "convergence_test(env, 10000, nn_policy, env, model, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f74f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
