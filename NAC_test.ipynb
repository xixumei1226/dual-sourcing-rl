{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "worldwide-prague",
   "metadata": {},
   "source": [
    "# Natural Actor-Critic for Long-Run Average Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-parks",
   "metadata": {},
   "source": [
    "Import packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "demanding-holiday",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import dual_sourcing\n",
    "from utility import convergence_test, evaluate\n",
    "from TBS_module import TBS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-replacement",
   "metadata": {},
   "source": [
    "Set up configurations and make an instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "capable-kingdom",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONFIG = {'Lr': 2, 'Le': 1, 'cr': 100, 'ce': 105, 'lambda': 2,\n",
    "          'h': 1, 'b': 19, 'starting_state': [0]*4, 'max_order': 4, 'max_inventory': 10\n",
    "env = gym.make('DualSourcing-v0', config=CONFIG)\n",
    "env.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-threshold",
   "metadata": {},
   "source": [
    "Define policy network class and value network class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "documentary-surrey",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not share the parameters\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_space = env.nA\n",
    "m = env.max_order + 1\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    # policy network\n",
    "    def __init__(self, hidden_size1, hidden_size2):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, action_space)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.action_output = nn.Softmax(dim = 0)\n",
    "        \n",
    "        self.save_actions = []\n",
    "\n",
    "    def forward(self, s):\n",
    "        out = self.fc1(s)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        action_probs = self.action_output(out)\n",
    "\n",
    "        return action_probs\n",
    "\n",
    "\n",
    "class Value(nn.Module):\n",
    "    # value network\n",
    "    def __init__(self, hidden_size1, hidden_size2):\n",
    "        super(Value, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.value_output = nn.Linear(hidden_size2, 1)\n",
    "\n",
    "    def forward(self, s):\n",
    "        out = self.fc1(s)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        value = self.value_output(out)\n",
    "\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casual-ensemble",
   "metadata": {},
   "source": [
    "Construct our networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "shaped-integer",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy(4, 8)\n",
    "value = Value(4, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-principal",
   "metadata": {},
   "source": [
    "Define auxiliary functions to initialize network, sample action, and return NN policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "quiet-tobago",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(layer):\n",
    "    # initialize network parameters\n",
    "    if type(layer) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(layer.weight)\n",
    "        layer.bias.data.fill_(0.01)\n",
    "\n",
    "def sample_action(state):\n",
    "    # sample action at current state according to policy network\n",
    "    state = torch.from_numpy(state).float()\n",
    "    action_probs = policy(state)\n",
    "    \n",
    "    dist = Categorical(action_probs)\n",
    "    action = dist.sample() \n",
    "    \n",
    "    action = np.asarray([action.item() // m, action.item() % m])\n",
    "    return action\n",
    "\n",
    "def nn_policy(env):\n",
    "    return sample_action(env.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-spouse",
   "metadata": {},
   "source": [
    "Use supervised learning to approximate a TBS policy as our initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3e1afbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(100)\n",
    "policy.apply(init_weights)\n",
    "\n",
    "a = 0.01\n",
    "maxit_init = 10000\n",
    "for i in range(maxit_init):\n",
    "    s = np.random.rand(4)\n",
    "    s = np.floor(s * np.asarray([5]*3 + [21])) \n",
    "    s[-1] -= 10\n",
    "    env.state = s\n",
    "    \n",
    "    qr, qe = TBS(env, 2, 2)\n",
    "    action = 5 * qr + qe\n",
    "    action = int(action)\n",
    "    probs = policy(torch.from_numpy(s).float())\n",
    "    \n",
    "    policy.zero_grad()\n",
    "    loss = (probs ** 2).sum() - 2 * probs[action]\n",
    "    loss.backward()\n",
    "    \n",
    "    for name, layer in policy.named_modules(): \n",
    "        if type(layer) == nn.Linear:\n",
    "            layer.weight.data -= a * layer.weight.grad\n",
    "            layer.bias.data -= a * layer.bias.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separated-topic",
   "metadata": {},
   "source": [
    "Assess initial policy performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "exposed-armenia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-217.985780210962, 2.382745749028857)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(env, 100, 5000, nn_policy, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-rugby",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# reset environment\n",
    "env.reset()\n",
    "\n",
    "state = env.state\n",
    "maxit = 500\n",
    "total_reward = 0\n",
    "J = 0\n",
    "z = 0\n",
    "Lambda = 0\n",
    "\n",
    "m = env.max_order + 1\n",
    "\n",
    "# initialize value network\n",
    "value.apply(init_weights)\n",
    "\n",
    "w = {}\n",
    "for name, layer in policy.named_modules():\n",
    "    if type(layer) == nn.Linear:\n",
    "        w[name+'_weight'] = torch.zeros(layer.weight.size())\n",
    "        w[name+'_bias'] = torch.zeros(layer.bias.size())\n",
    "\n",
    "z = {}\n",
    "for name, layer in value.named_modules():\n",
    "    if type(layer) == nn.Linear:\n",
    "        z[name+'_weight'] = torch.zeros(layer.weight.size())\n",
    "        z[name+'_bias'] = torch.zeros(layer.bias.size())\n",
    "\n",
    "# step size\n",
    "alpha0 = 1e-1\n",
    "beta0 = 1e-2\n",
    "alphac = 1000\n",
    "betac = 100000\n",
    "c = 0.8\n",
    "\n",
    "# use multiple epochs of TD learning\n",
    "TD_epochs = 10\n",
    "\n",
    "for i in range(maxit):\n",
    "\n",
    "    # step size calculations\n",
    "    alpha = alpha0 * alphac / (alphac + i**(2/3))\n",
    "    beta = beta0 * betac / (betac + i)\n",
    "    xi = c * alpha\n",
    "    \n",
    "    # draw action\n",
    "    state = torch.from_numpy(state).float()\n",
    "    action_probs = policy(state)\n",
    "    dist = Categorical(action_probs)\n",
    "    action = dist.sample() \n",
    "    \n",
    "    # get next state & reward\n",
    "    next_state, reward, _, _ = env.step(np.asarray([action.item() // m, action.item() % m]))\n",
    "    next_state = torch.from_numpy(next_state).float()\n",
    "    \n",
    "    # value nn back prop\n",
    "    value.zero_grad()\n",
    "    value_loss = value(state)\n",
    "    value_loss.backward()\n",
    "    \n",
    "    # TD learning with multiple epochs\n",
    "    state_prime = np.asarray(state, dtype = int).copy()\n",
    "    state_prime = torch.from_numpy(state_prime).float()\n",
    "    next_state_prime = np.asarray(next_state, dtype = int).copy()\n",
    "    next_state_prime = torch.from_numpy(next_state_prime).float()\n",
    "    delta = 0\n",
    "    for j in range(TD_epochs):\n",
    "        \n",
    "        # update average reward\n",
    "        J = J + xi * (reward - J)\n",
    "        # update temporal difference\n",
    "        delta = delta + reward - J + value(next_state_prime).item() - value(state_prime).item()\n",
    "        \n",
    "        # enter next state\n",
    "        state_prime = np.asarray(next_state_prime, dtype = int).copy()\n",
    "        state_prime = torch.from_numpy(state_prime).float()\n",
    "        \n",
    "        # draw action\n",
    "        action_probs = policy(state_prime)\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample() # 0, 1, ..., 24\n",
    "        \n",
    "        # draw next state and collect reward\n",
    "        next_state_prime, reward, _, _ = env.step(np.asarray([action.item() // m, action.item() % m]))\n",
    "        next_state_prime = torch.from_numpy(next_state_prime).float()\n",
    "     \n",
    "    # !! reset environment state back to where we started\n",
    "    env.state = next_state \n",
    "    \n",
    "    # average TD\n",
    "    delta = delta / TD_epochs\n",
    "    \n",
    "    # critic update\n",
    "    stepsize = alpha * delta\n",
    "    for name, layer in value.named_modules():\n",
    "        if type(layer) == nn.Linear:\n",
    "            z[name+'_weight'] = Lambda * z[name+'_weight'] + layer.weight.grad\n",
    "            z[name+'_bias'] = Lambda * z[name+'_bias'] + layer.bias.grad\n",
    "            layer.weight.data += stepsize * z[name+'_weight']\n",
    "            layer.bias.data += stepsize * z[name+'_bias']\n",
    "    \n",
    "    # actor update\n",
    "    psi_wt = 0\n",
    "    for name, layer in policy.named_modules():\n",
    "        if type(layer) == nn.Linear:\n",
    "            psi_wt += (layer.weight.grad * w[name+'_weight']).sum().item()\n",
    "            psi_wt += (layer.bias.grad * w[name+'_bias']).sum().item()\n",
    "    \n",
    "    stepsize = alpha * (delta - psi_wt)\n",
    "    for name, layer in policy.named_modules():\n",
    "        if type(layer) == nn.Linear:\n",
    "            w[name+'_weight'] += stepsize * layer.weight.grad\n",
    "            w[name+'_bias'] += stepsize * layer.bias.grad\n",
    "            layer.weight.data += beta * w[name+'_weight']\n",
    "            layer.bias.data += beta * w[name+'_bias']\n",
    "            \n",
    "            layer.weight.data = torch.clamp(layer.weight.data, min=-1, max=1)\n",
    "            layer.bias.data = torch.clamp(layer.bias.data, min=-1, max=1)\n",
    "\n",
    "    # transition to next state\n",
    "    state = np.asarray(next_state, dtype = int).copy()\n",
    "   \n",
    "    print('\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aging-italic",
   "metadata": {},
   "source": [
    "Assess output policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "recovered-summit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-218.25038327404772, 1.9000560543719498)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(env, 100, 5000, nn_policy, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soviet-desperate",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The algorithm does not provide significant improvement upon the initial policy. We suspect that the value function is relative in the sense that its accuracy relies on the accuracy of the estimate of the long-run average reward, making it highly unstable and extremely difficult to learn.\n",
    "\n",
    "In the future, it might be worthwhile to investigate how to adaptively tune the step sizes and employ other tricks to stablize the algorithm and boost learning. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
